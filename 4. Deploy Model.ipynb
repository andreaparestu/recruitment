{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <p style=\"text-align: center;\">Modul 4\n",
    "##  <p style=\"text-align: center;\">Deploy Model\n",
    "### <p style=\"text-align: center;\"> Human Capital Data Science (HCDS)\n",
    "\n",
    "# 4.1. Otomisasi Alur Kerja Machine Learning\n",
    "Pada *Machine Learning* terdapat alur kerja yang dapat diotomatisasi. Hal ini dapat dilakukan menggunakan Pipeline pada Scikit-learn Python. Python scikit-learn menyediakan utilitas Pipeline untuk membantu mengotomatisasi alur kerja pembelajaran mesin\n",
    "\n",
    "Dalam bagian ini akan dibahas mengenai:\n",
    "1. Cara menggunakan *pipeline* untuk meminimalkan kebocoran data.\n",
    "2. Bagaimana membangun persiapan data dan pemodelan *pipeline*.\n",
    "3. Bagaimana membangun *feature extraction* dan pemodelan *pipeline*.\n",
    "\n",
    "*Pipeline* akan dapat digunakan ketika dalam proses pemodelan *machine learning* dimungkinkan pembuatan rangkaian linear dari  transoformasi data yang dapat dievaluasi tingkat kinerjanya. \n",
    "\n",
    "Tujuan utama dari otomisasi alur kerja adalah untuk memastikan dataset dapat digunakan dalam *pipeline* yang tersedia untuk evaluasi, seperti train dataset atau setiap *fold* pada prosedur *cross validation*. Berikutnya juga dapat dilakukan peninjauan dokumentasi API untuk kelas *Pipeline* & ***FeatureUnion*** dan *pipeline* modul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1. Persiapan Data dan Modeling Pipeline \n",
    "Dalam menerapkan *machine learning* seringkali terjadi kebocoran data dalam proses pemodelan, seperti terjadinya penggunaan test data pada train data. Oleh karena itu diperlukan alat uji pemisahan dataset yang cukup kuat agar tidak terjadi kebocoran tersebut.  Persiapan data adalah salah satu cara mudah untuk membocorkan pengetahuan dari seluruh dataset pelatihan ke algoritma. \n",
    "\n",
    "Sebagai contoh, menyiapkan data Anda menggunakan normalisasi atau standardisasi pada seluruh dataset pelatihan sebelum belajar tidak akan menjadi tes yang valid karena train data akan dipengaruhi oleh skala data dalam test data. \n",
    "\n",
    "*Pipeline* dapat mencegah terjadinya kebocoran data dari test dataset yang digunakan dengan memastikan bahwa persiapan data seperti standardisasi dibatasi untuk setiap *fold* pada *cross validation*. Contoh di bawah ini menunjukkan persiapan data penting dan evaluasi model aliran kerja pada dataset HR. \n",
    "\n",
    "*Pipeline* didefinisikan dengan dua langkah:\n",
    "1. Standarisasi data.\n",
    "2. Mempelajari model *Decision Tree Classifier* (algoritma terbaik untuk dataset HR).\n",
    "Pipa kemudian dievaluasi menggunakan 10 kali lipat lintas-validasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973978315262719\n"
     ]
    }
   ],
   "source": [
    "# Pembuatan sebuah Pipeline yang menstandarisasi dataset kemudian membuat sebuah model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Pembuatan pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('CART', DecisionTreeClassifier()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# Evaluasi pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bagaimana kami membuat daftar langkah-langkah Python yang disediakan untuk Saluran Pipa untuk memproses data. Perhatikan juga bagaimana Pipeline itu sendiri diperlakukan seperti estimator dan dievaluasi secara keseluruhan dengan prosedur k-fold cross-validation. Menjalankan contoh memberikan ringkasan keakuratan pengaturan pada dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2. *Feature Extraction* dan *Modeling Pipeline*\n",
    "Ekstraksi fitur adalah prosedur lain yang rentan terhadap kebocoran data. Seperti persiapan data, prosedur ekstraksi fitur harus dibatasi pada data dalam dataset pelatihan Anda. Pipeline menyediakan alat yang berguna yang disebut FeatureUnion yang memungkinkan hasil seleksi berbagai fitur dan prosedur ekstraksi untuk digabungkan menjadi dataset yang lebih besar di mana model dapat dilatih. Yang penting, semua ekstraksi fitur dan penyatuan fitur terjadi dalam setiap lipatan prosedur validasi silang. Contoh di bawah ini menunjukkan pipeline yang didefinisikan dengan empat langkah:1. Ekstraksi Fitur dengan Analisis Komponen Utama (3 fitur).2. Ekstraksi Fitur dengan Seleksi Statistik (6 fitur).3. Uni Fitur.4. Pelajari Model Regresi Logistik.Pipa kemudian dievaluasi menggunakan 10 kali lipat lintas-validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8977481234361969\n"
     ]
    }
   ],
   "source": [
    "# Pembuatan Pipeline yang melakukan extract feature dari data kemudian membuat model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bagaimana FeatureUnion adalah Pipeline-nya sendiri yang pada gilirannya adalah satu langkah di Pipeline akhir yang digunakan untuk memberi umpan Logistic Regression. Ini mungkin membuat Anda berpikir tentang bagaimana Anda dapat mulai menanamkan saluran pipa dalam jaringan pipa. Menjalankan contoh memberikan ringkasan keakuratan pengaturan pada dataset.\n",
    "\n",
    "#### Ringkasan\n",
    "Dalam bab ini Anda menemukan berbagai kebocoran data dalam pembelajaran mesin terapan. Anda menemukan utilitas Saluran Pipa di Python scikit-belajar dan bagaimana mereka dapat digunakan untuk mengotomatisasi kerja pembelajaran mesin diterapkan standar ¬. Anda telah mempelajari cara menggunakan Saluran Pipa dalam dua kasus penggunaan penting:Âˆ Persiapan data dan pemodelan dibatasi untuk setiap lipatan prosedur validasi silang.Âˆ Ekstraksi fitur dan penyatuan fitur yang dibatasi untuk setiap lipatan prosedur validasi silang.\n",
    "\n",
    "Ini melengkapi pelajaran tentang cara mengevaluasi algoritma pembelajaran mesin. Dalam pelajaran berikutnya Anda akan melihat pertama Anda bagaimana meningkatkan kinerja algoritma pada masalah Anda dengan menggunakan metode ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2. Peningkatan Kinerja dengan Ensemble\n",
    "\n",
    "Metode Ensemble merupakan salah satu metode yang dapat digunakan untuk meningkatkan kinerja algoritma *machine learning* agar dapat menghasilkan model prediksi dengan tingkat akurasi yang leih tinggi. \n",
    "\n",
    "Pada bagian ini akan dibahas mengenai beberapa metode Ensemble yang terdapat pada scikit-learn Python, yaitu *Boosting, Bagging, dan Majority Voting.* Untuk lebih detailnya akan dibahas mengenai beberapa hal berikut ini:\n",
    "1. Cara menggunakan metode *bagging ensemble* seperti pohon keputusan yang dikantongi, hutan acak dan pohon tambahan.\n",
    "2. Cara menggunakan metode *boosting ensemble* seperti AdaBoost dan peningkatan gradien stochastic.\n",
    "3. Cara menggunakan metode *voting ensemble* untuk menggabungkan prediksi dari beberapa algoritma.## 8.1. Tipe Penggabungan Model dalam *Ensemble Prediction*.\n",
    "\n",
    "Tiga metode ensemble dalam scikit-learn yang dapat digunakan untuk menggabungkan algoritma *machine-learning* yang berbeda adalah:\n",
    "1. ***Bagging Ensemble Method*** - membangun beberapa model (biasanya dari jenis yang sama) dari subsamples berbeda yang dipecah dari dataset yang digunakan.\n",
    "2. ***Boosting Ensemble Method*** - membangun urutan beberapa model (biasanya dari jenis yang sama) dimana masing-masing model merupakan perbaikan kesalahan prediksi dari model sebelumnya dalam sebuah rangkaian urutan model.\n",
    "3. ***Voting Ensemble Method***. Membangun beberapa model (biasanya dari jenis yang berbeda) dan statistik sederhana (seperti menghitung mean) digunakan untuk menggabungkan prediksi.\n",
    "\n",
    "Ini mengasumsikan Anda pada umumnya akrab dengan algoritme pembelajaran mesin dan metode ensembel dan tidak akan membahas detail cara kerja atau parameternya. Pima Indian onset of diabetes dataset digunakan untuk mendemonstrasikan setiap algoritma. Setiap ensemble algorithm didemonstrasikan menggunakan 10-fold cross-validation dan metrik kinerja akurasi klasi ﬁ kasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1. Bagging Ensemble\n",
    "Bootstrap Aggregation (atau Bagging) melibatkan pengambilan beberapa sampel dari dataset pelatihan Anda (dengan penggantian) dan pelatihan model untuk setiap sampel. Prediksi output ¬ nal adalah rata-rata di seluruh prediksi dari semua sub-model. Tiga model bagging yang dibahas di bagian ini adalah sebagai berikut:\n",
    "- Bagged Decision Trees.\n",
    "- Random Forest.\n",
    "- Extra Trees\n",
    "\n",
    "### a. Bagged Decision Trees\n",
    "Pengepakan berkinerja terbaik dengan algoritme yang memiliki ***variansi data tinggi***. Contoh populer adalah *Decision Tree*, sering dibangun tanpa pemangkasan. Dalam contoh di bawah ini adalah contoh penggunaan BaggingClassifier dengan algoritma Klasifikasi dan Regresi Pohon (DecisionTreeClassifier1). Sebanyak 100 pohon dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896580483736447\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Modeling menggunakan Bagged Decision Tree\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Random Forest\n",
    "\n",
    "Random Forests adalah perpanjangan dari pohon keputusan yang dikantongi. Sampel dari dataset pelatihan diambil dengan penggantian, tetapi pohon-pohon dibangun dengan cara yang mengurangi korelasi antara klasifikasi individu. Secara khusus, alih-alih dengan rakus memilih titik perpecahan terbaik dalam konstruksi setiap pohon, hanya satu bagian acak dari fitur yang dipertimbangkan untuk setiap pemisahan. Anda dapat membuat model Random Forest untuk klasifikasi menggunakan kelas RandomForestClassifier2. Contoh di bawah ini menunjukkan penggunaan Random Forest untuk klasifikasi dengan 100 pohon dan titik terpisah yang dipilih dari seleksi acak 3 fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990909090909091\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Modeling menggunakan Random Forest\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Extra Trees\n",
    "Pohon Ekstra adalah modifikasi lain dari pengepakan di mana pohon-pohon acak dibangun dari sampel dataset pelatihan. Anda dapat membangun model Pohon Ekstra untuk klasifikasi menggunakan kelas ExtraTreesClassifier3. Contoh di bawah ini memberikan demonstrasi pohon tambahan dengan jumlah pohon yang diatur ke 100 dan perpecahan yang dipilih dari 7 fitur acak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9889908256880734\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Modeling menggunakan Extra Trees\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. Boosting  Ensemble Algorithms\n",
    "\n",
    "***Boosting Ensemble Algorithms*** merupakan suatu *workflow* yang membentuk suatu rangkaian model machine learning dimana  model tersebut terus diperbaiki berdasarkan tingkat akurasi dari model sebelumnya dalam rangkaian model tersebut. Sehingga akan diperoleh suatu model machine learning akhir yang merupakan rangkaian dari seluruh model pada rangkaian ensemble, dimana setiap model memiliki bobot yang berbeda pada pembentukan model machine learning tersebut. Dua algoritma boosting ensemble - machine learning  ensembel yang paling umum adalah:\n",
    "- AdaBoost\n",
    "- Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Adaboost\n",
    "AdaBoost mungkin merupakan algoritma ensemble pendongkrak pertama yang berhasil. Ini umumnya bekerja dengan membobotkan instance dalam dataset dengan cara mudah atau sulitnya mengklasifikasi, memungkinkan algoritma untuk kurang memperhatikannya dalam pembuatan model berikutnya. Anda dapat membangun model AdaBoost untuk klasifikasi menggunakan kelas AdaBoostClassifier4. Contoh di bawah ini menunjukkan pembangunan 30 pohon keputusan secara berurutan menggunakan algoritma AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9539616346955796\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "num_trees = 30\n",
    "seed=7\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting (juga disebut Gradient Boosting Machines) adalah salah satu teknik ensemble yang paling canggih. Ini juga merupakan teknik yang terbukti menjadi salah satu teknik terbaik yang tersedia untuk meningkatkan kinerja melalui ansambel. Anda dapat membangun model Gradient Boosting untuk klasifikasi menggunakan ***GradientBoostingClassifier*** class. Contoh di bawah ini menunjukkan Stochastic Gradient Boosting untuk klasifikasi dengan 100 pohon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731442869057547\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3. Voting Ensemble \n",
    "Voting adalah salah satu cara paling sederhana untuk menggabungkan prediksi dari berbagai algoritma pembelajaran mesin. Ini bekerja dengan terlebih dahulu membuat dua atau lebih model mandiri dari set data pelatihan Anda. Klasifikasi Voting kemudian dapat digunakan untuk membungkus model Anda dan rata-rata prediksi sub-model ketika diminta untuk membuat prediksi untuk data baru. Prediksi dari sub-model dapat ditimbang, tetapi menentukan bobot untuk klasifikasi secara manual atau bahkan secara heuristik sulit. \n",
    "\n",
    "Metode yang lebih maju dapat mempelajari cara terbaik untuk memperkirakan prediksi dari sub-model, tetapi ini disebut susun (agregasi bertumpuk) dan saat ini tidak disediakan dalam scikit-learn. Anda dapat membuat model ensemble pemungutan suara untuk klasifikasi menggunakan kelas VotingClassifier6. Kode di bawah ini memberikan contoh menggabungkan prediksi regresi logistik, pohon klasifikasi dan regresi dan mendukung mesin vektor bersama-sama untuk masalah klasifikasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9492910758965805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ringkasan\n",
    "Dalam bab ini Anda menemukan algoritma pembelajaran mesin ensembel untuk meningkatkan kinerja model pada masalah Anda. Anda belajar tentang:\n",
    "- *Ensemble Bagging* termasuk *Bagged Decision Tree, Random Forest dan Extra Trees.*\n",
    "- *Boosing Ensamble* termasuk *AdaBoost dan Stochastic Gradient Boosting*.\n",
    "- *Voting Ensemble* untuk membuat rata-rata prediksi untuk model apa pun yang digunakan dalam pemodelan.\n",
    "\n",
    "Pada bagian selanjutnya Anda akan menemukan teknik lain yang dapat Anda gunakan untuk meningkatkan kinerja algoritma pada dataset Anda yang disebut tuning algoritme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. Algorithm Tuning\n",
    "\n",
    "Setiap algoritma *machine learning* memiliki parameter sehingga penggunaannya dapat disesuaikan untuk masalah data yang diberikan. Model dapat memiliki banyak parameter dan kombinasi terbaik dari parameter dapat diperlakukan sebagai masalah pencarian. \n",
    "\n",
    "Dalam bab ini akan dijelaskan mengenai cara menyesuaikan (*tuning*) parameter algoritma *machine learning* dengan *scikit-learn* pada Python. Berikut materi yang akan dijelaskan:\n",
    "1. Pentingnya penyetelan parameter algoritma untuk meningkatkan kinerja algoritma.\n",
    "2. Cara menggunakan strategi tuning algoritma pencarian jaringan.\n",
    "3. Cara menggunakan strategi tuning algoritma pencarian acak.\n",
    "\n",
    "Penalaan algoritma adalah langkah terakhir dalam proses pembelajaran mesin yang diterapkan sebelum memperbaiki model Anda. Kadang-kadang disebut optimasi hiperparameter di mana parameter algoritma disebut sebagai hiperparameter, sedangkan koefisien yang ditemukan oleh algoritma pembelajaran mesin itu sendiri disebut sebagai parameter. Optimalisasi menyarankan sifat pencarian masalah. \n",
    "\n",
    "Berurutan sebagai masalah pencarian, Anda dapat menggunakan strategi pencarian yang berbeda untuk menemukan parameter yang baik dan kuat atau set parameter untuk algoritma pada masalah yang diberikan. Python scikit-learn menyediakan dua metode sederhana untuk penyetelan parameter algoritma:\n",
    "- ***Grid Search Parameter Tuning.***\n",
    "- ***Random Search Parameter Tuning.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. Grid Serach Parameter Tuning\n",
    "Pencarian grid adalah pendekatan untuk penyetelan parameter yang secara sistematis akan membangun dan mengevaluasi model untuk setiap kombinasi parameter algoritma yang ditentukan dalam sebuah grid. Anda dapat melakukan pencarian kisi menggunakan GridSearchCV class1. Contoh di bawah ini mengevaluasi nilai alpha yang berbeda untuk algoritma Ridge Regression pada dataset diabetes standar. Ini adalah pencarian grid satu dimensi.\n",
    "\n",
    "Menjalankan contoh mencantumkan skor optimal yang dicapai dan set parameter dalam kisi yang mencapai skor itu. Dalam hal ini nilai alfa 0,0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23520656853454305\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. Random Search Parameter Tuning\n",
    "\n",
    "Pencarian acak adalah pendekatan untuk penyetelan parameter yang akan mengambil sampel parameter algoritma dari distribusi acak (mis. Seragam) untuk sejumlah iterasi tetap. Model dibangun dan dievaluasi untuk setiap kombinasi parameter yang dipilih. Anda dapat melakukan pencarian acak untuk parameter algoritma menggunakan Random2SearchCV class2. Contoh di bawah ini mengevaluasi nilai alpha acak yang berbeda antara 0 dan 1 untuk algoritma Ridge Regression pada dataset diabetes standar. Total 100 iterasi dilakukan dengan nilai alpha acak seragam yang dipilih dalam kisaran antara 0 dan 1 (kisaran yang dapat diambil nilai alpha).\n",
    "\n",
    "Menjalankan contoh menghasilkan hasil seperti yang ada dalam contoh pencarian kotak di atas. Nilai alpha optimal dekat 0.0 ditemukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2352063034368106\n",
      "0.0014268805627581926\n"
     ]
    }
   ],
   "source": [
    "# Randomized for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "param_grid = {'alpha': uniform()}\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X, Y)\n",
    "\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ringkasan\n",
    "\n",
    "Penyesuaian parameter algoritma adalah langkah penting untuk meningkatkan kinerja algoritma tepat sebelum menyajikan hasil atau menyiapkan sistem untuk produksi. Dalam bab ini Anda menemukan penyetelan parameter algoritma dan dua metode yang dapat Anda gunakan sekarang di Python dan scikit-belajar untuk meningkatkan hasil algoritma Anda:\n",
    "- Penyetelan Parameter Pencarian Grid\n",
    "- Pencarian Parameter Acak\n",
    "\n",
    "Pelajaran ini menyimpulkan cakupan teknik yang dapat Anda gunakan untuk meningkatkan kinerja algoritma pada dataset Anda. Dalam pelajaran berikutnya dan terakhir Anda akan menemukan bagaimana Anda dapat menyelesaikan model Anda untuk menggunakannya pada data yang tidak terlihat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4. Save and Load Machine Learning Models\n",
    "\n",
    "Setelah berhasil menemukan model machine learning terbaik, maka hal berikutnya yang perlu dilakukan adalah menyimpan model tersebut dan memuat kembali ketika ingin dilakukan prediksi menggunakan model tersebut di kemudian hari. \n",
    "\n",
    "Pada bagian ini akan dibahas mengenai:\n",
    "1. Pentingnya model serialisasi untuk digunakan kembali.\n",
    "2. Cara menggunakan ***Pickle*** untuk membuat serial dan deserialize model *machine learning*.\n",
    "3. Cara menggunakan ***Joblib*** untuk membuat serial dan deserialize model *machine learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1. Finilaze Model with Pickle\n",
    "\n",
    "***Pickle*** adalah cara standar untuk membuat objek bersambung dalam Python. Operasi *pickle* akan membantu membuat rangkaian algoritma *machine learning* dan menyimpannya ke dalam sebuah ﬁle. Ketika model ingin digunakan kembali, maka *pickle file* tersebut dapat digunakan untuk membuat prediksi baru. \n",
    "\n",
    "Berikut ini adalah contoh pembuatan model prediksi pada data HR dengan menggunakan algoritma Random Forest, lalu menyimpan model tersebut untuk dapat memprediksi dataset terbaru. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9909924937447873\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pickle import dump \n",
    "from pickle import load\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Modeling menggunakan Random Forest\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "# Penyimpanan Model Machine Learning\n",
    "namafile = 'hr_rf_model.sav'\n",
    "dump(model, open(namafile,'wb'))\n",
    "\n",
    "# Pemuatan Model Machine Learning\n",
    "loaded_model = load(open(namafile, 'rb')) \n",
    "\n",
    "# Uji Model dengan K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(loaded_model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2. Finalize Model with Joblib\n",
    "\n",
    "***Joblib*** adalah bagian dari ekosistem SciPy dan menyediakan utilitas untuk pipelining pekerjaan Python. *Library joblib* menyediakan utilitas untuk menyimpan dan memuat objek Python yang memanfaatkan struktur data NumPy secara efisien. Hal ini dapat berguna untuk beberapa algoritma pembelajaran mesin yang membutuhkan banyak parameter atau menyimpan seluruh dataset (mis. K-Nearest Neighbors). \n",
    "\n",
    "Berikut ini adalah contoh pembuatan model prediksi pada data HR dengan menggunakan algoritma Random Forest, lalu menyimpan model tersebut menggunakan *joblib* untuk dapat memprediksi dataset terbaru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9911592994161801\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals.joblib import dump \n",
    "from sklearn.externals.joblib import load \n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Modeling menggunakan Random Forest\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "# Penyimpanan Model Machine Learning\n",
    "namafile = 'hr_rf_model.sav'\n",
    "dump(model, open(namafile,'wb'))\n",
    "\n",
    "# Pemuatan Model Machine Learning\n",
    "loaded_model = load(open(namafile, 'rb')) \n",
    "\n",
    "# Uji Model dengan K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(loaded_model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
