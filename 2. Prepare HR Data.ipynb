{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <p style=\"text-align: center;\">Modul 2\n",
    "##  <p style=\"text-align: center;\">Preparte HR Data\n",
    "### <p style=\"text-align: center;\"> Human Capital Data Science (HCDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banyak algoritma machine learning akan membuat perkiraan tentang dataset yang diinput ke dalam sistem. Dalam bab ini akan dijelaskan mengenai cara menyiapkan dataset untuk machine learning menggunakan Python menggunakan scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Data Transforms\n",
    "## 2.1.1 Rescale Data\n",
    "\n",
    "Standarisasi Data adalah teknik yang berguna untuk mengubah variable yang memiliki distribusi dengan rata-rata dan standar deviasi yang berbeda-beda, menjadi distribusi standar dengan rata-rata 0 dan standar deviasi 1. \n",
    "\n",
    "Beberapa algoritma akan memiliki kinerja yang lebih baik ketika menggunakan variable berdistribusi standar, seperti ***linear regression***, ***logistic regression***, dan analisa ***linear discriminate***. \n",
    "\n",
    "Standarisasi data dapat dilakukan menggunakan ***scikit-learn*** dengan ***StandardScaler*** *class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78  0.781 0.6   0.776 0.5   0.    0.    0.    0.5  ]\n",
      " [0.022 0.812 1.    0.822 0.25  0.    0.    0.    0.5  ]\n",
      " [0.692 0.797 0.6   0.593 0.375 0.    0.    0.    0.   ]\n",
      " [0.011 0.641 0.8   0.706 0.25  0.    0.    0.    0.   ]\n",
      " [0.912 0.766 0.6   0.762 0.375 0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "import pandas as pd\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv \n",
    "from numpy import set_printoptions \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load Dataset\n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable \n",
    "X = array[:,0:9] \n",
    "Y = array[:,9] \n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "rescaledX = scaler.fit_transform(X) \n",
    "\n",
    "# summarize transformed data \n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2. Standardize Data\n",
    "\n",
    "Standarisasi Data adalah teknik yang berguna untuk mengubah variable yang memiliki distribusi dengan rata-rata dan standar deviasi yang berbeda-beda, menjadi distribusi standar dengan rata-rata 0 dan standar deviasi 1. \n",
    "\n",
    "Beberapa algoritma akan memiliki kinerja yang lebih baik ketika menggunakan variable berdistribusi standar, seperti ***linear regression***, ***logistic regression***, dan analisa ***linear discriminate***. \n",
    "\n",
    "Standarisasi data dapat dilakukan menggunakan ***scikit-learn*** dengan ***StandardScaler*** *class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.633  0.625  0.765  1.027  1.589 -0.423 -0.151 -1.194  0.608]\n",
      " [-2.054  0.75   2.585  1.239  0.272 -0.423 -0.151 -1.194  0.608]\n",
      " [ 0.321  0.687  0.765  0.197  0.93  -0.423 -0.151 -1.194 -0.951]\n",
      " [-2.093  0.061  1.675  0.707  0.272 -0.423 -0.151 -1.194 -0.951]\n",
      " [ 1.1    0.562  0.765  0.963  0.93  -0.423 -0.151 -1.194 -0.951]]\n"
     ]
    }
   ],
   "source": [
    "# Standarisasi Data (Rata-rata = 0 dan Standar Deviasi = 1)\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv \n",
    "from numpy import set_printoptions \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Dataset\n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable \n",
    "X = array[:,0:9] \n",
    "Y = array[:,9] \n",
    "standardizer = StandardScaler().fit(X) \n",
    "standardizedX = standardizer.transform(X) \n",
    "\n",
    "# summarize transformed data \n",
    "set_printoptions(precision=3) \n",
    "print(standardizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Normalize Data\n",
    "Normalisasi dalam *scikit-learn* mengacu pada penyeleksian kembali setiap observasi (baris) untuk memiliki panjang 1 (disebut norma satuan atau vektor dengan panjang 1 dalam aljabar linear). \n",
    "\n",
    "Metode pre-processing ini dapat berguna untuk ***dataset sparse*** (banyak nol) dengan variable dari berbagai skala ketika menggunakan algoritma yang nilai-nilai masukan berat seperti algoritma *Neural Network* dan algoritma yang menggunakan ukuran jarak seperti algoritma ***k-Nearest Neighbors (k-NN)***. Normalisasi data pada Python dengan *scikit-learn* dapat menggunakan kelas ***Normalizer***.\n",
    "\n",
    "#### Dataset HR\n",
    "\n",
    "Contoh normalisasi variable pada dataset *datahr.csv* dapat dilihat pada bagian berikut ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.052e-03 3.281e-03 1.907e-02 9.995e-01 2.289e-02 0.000e+00 0.000e+00\n",
      "  3.815e-03 7.630e-03]\n",
      " [4.042e-04 3.234e-03 2.572e-02 9.995e-01 1.470e-02 0.000e+00 0.000e+00\n",
      "  3.675e-03 7.349e-03]\n",
      " [3.227e-03 3.899e-03 2.241e-02 9.995e-01 2.241e-02 0.000e+00 0.000e+00\n",
      "  4.482e-03 4.482e-03]\n",
      " [4.047e-04 3.116e-03 2.428e-02 9.996e-01 1.619e-02 0.000e+00 0.000e+00\n",
      "  4.047e-03 4.047e-03]\n",
      " [3.551e-03 3.281e-03 1.930e-02 9.996e-01 1.930e-02 0.000e+00 0.000e+00\n",
      "  3.859e-03 3.859e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Normalisasi Data (Panjang data adalah 1)\n",
    "# Load Library\n",
    "from pandas import read_csv \n",
    "from numpy import set_printoptions \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Load Dataset\n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable \n",
    "X = array[:,0:9] \n",
    "Y = array[:,9]\n",
    "normalizerX = Normalizer().fit(X)\n",
    "normalizedX = normalizerX.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4 Binarize Data\n",
    "\n",
    "Apabila diperlukan penambahan variable baru yang bersifat binary untuk mempertajam pengambilan keputusan dari hasil analisa data, scikit-learn dapat digunakan melalui ***Binarizer*** class. Penambahan variable binary tersebut juga dapat menjadi output atau target variable dalam pengolah data *machine learning*.\n",
    "\n",
    "#### Data Set Pima Indians Diabetes\n",
    "Contoh perubahan dataset *pima-indians-diabetes.data.csv* menjadi binary data adalah sebagai berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Binarisasi Data\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv \n",
    "from numpy import set_printoptions \n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "binarizerX = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizerX.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "Pada bagian ini telah dijelaskan mengenai *data trasnform* untuk Machine Learning dengan Python menggunakan *scikit-learn*, yaitu sebagai berikut:\n",
    "- Rescale data.\n",
    "- Standardize data.\n",
    "- Normalize data.\n",
    "- Binarize data\n",
    "\n",
    "Dalam pelajaran berikutnya akan dijelaskan mengenai cara memilih *variable* dari dataset yang paling relevan untuk membuat model prediksi *machine learning*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Feature Selection\n",
    "\n",
    "Variable dataset yang digunakan untuk melatih model *machine learning* memiliki pengaruh besar pada tingkat akurasi dari model prediksi yang dihasilkan. Variable yang tidak relevan atau hanya relevan sebagian dapat berdampak negatif pada kinerja model. \n",
    "\n",
    "Dalam bagian ini akan dijelaskan mengenai teknik *feature selection* secara otomatis yang dapat digunakan untuk menyiapkan data *machine learning* pada Python dengan *scikit-learn*. \n",
    "\n",
    "Setelah menyelesaikan pelajaran ini, Anda akan tahu cara menggunakan:\n",
    "1. ***Univariate Selection*** \n",
    "2. ***Recursive Feature Elimination***\n",
    "3. ***Principle Component Analysis***\n",
    "4. ***Feature Importance***\n",
    "\n",
    "***Feature Selection*** adalah proses pemilihan *input variable* yang relevan dalam memprediksi *output variable* secara otomatis. \n",
    "\n",
    "Memiliki input variabel yang tidak relevan terhadap prediksi output variable dapat menurunkan tingkat akurasi model prediksi pada machine learning, terutama algoritma linier seperti ***linier regression dan logistic regression***. \n",
    "\n",
    "Tiga manfaat dari melakukan *feature selection* sebelum pemodelan data adalah:\n",
    "- ***Mengurangi Overfitting***: Data relevan dapat membantu mengurangi pengaruh pengambilan keputusan berdasarkan data yang tidak relevan.\n",
    "- ***Meningkatkan Akurasi***: Data relevan dapat membantu meingkatkan akurasi pemodelan data.\n",
    "- ***Mengurangi Waktu Pemodelan Data***: Semakin efisien jumlah data yang diolah (input variable yang kurang relevan dikurangi), maka semakin sedikit juga waktu yang diperlukan machine learning untuk pemodelan data.  \n",
    "\n",
    "Berikut ini akan dipejari mengenai berbagai metode dalam melakukan *feature selection* dalam machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Univariate Selection\n",
    "\n",
    "Tes statistik dapat digunakan untuk memilih *input variable* yang memiliki hubungan terkuat dengan *output variable*. \n",
    "\n",
    "*Library scikit-learn* menyediakan kelas ***SelectKBest*** yang dapat digunakan dengan beberapa uji statistik yang berbeda untuk memilih sejumlah input variable tertentu. \n",
    "\n",
    "Contoh di bawah ini menggunakan uji statistik ***chi-square*** untuk *non-negative input variable* untuk memilih beberapa input variable terbaik dari dataset yang digunakan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV']\n",
      "[  117.341    41.199   690.534 19580.099   601.153   177.457    39.201\n",
      "    20.778    59.323]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction dengan Univariate Statistical Tests (Chi-squared for classification)\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(nama[0:8])\n",
    "print(fit.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terlihat keempat *input variable* yang memiliki feature selection score tertinggi adalah variable:\n",
    "1. KB (Kerja_bulanan): Rata-rata jam bekerja karyawan pada tahun terakhir. \n",
    "2. PROJ (Proyek): Jumlah proyek yang dikerjakan pada tahun terakhir. \n",
    "3. DK (Durasi_kerja): Jumlah tahun bekerja karyawan selama di perusahaan. \n",
    "4. KK (Kepuasan_kerja): Tingkat kepuasan karyawan terhadap pekerjaan saat ini. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5. 262.   6.   0.]\n",
      " [  7. 272.   4.   0.]\n",
      " [  5. 223.   5.   0.]\n",
      " [  6. 247.   4.   0.]\n",
      " [  5. 259.   5.   0.]]\n"
     ]
    }
   ],
   "source": [
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Recursive Feature Elimination\n",
    "\n",
    "***The Recursive Feature Elimination*** (atau RFE) bekerja dengan menghapus input variable secara berulang dan membangun model berdasarkan input variable yang tersisa. \n",
    "\n",
    "RFE menggunakan akurasi model untuk mengidentifikasi input variable yang paling berkontribusi dalam meningkatkan akurasi pemodelan output variable. Anda dapat mempelajari lebih lanjut tentang kelas RFE pada dokumentasi scikit-learn. \n",
    "\n",
    "Contoh di bawah ini menggunakan RFE dengan algoritma ***logistic Regression*** untuk memilih 3 input variable yang paling berkontribusi dalam meningkatkan akurasi pemodelan output variable ***BUY*** pada dataset ***browsing.csv***. Pemilihan algoritma tidak terlalu penting asalkan terampil dan konsisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Feature Name: ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV']\n",
      "Selected Features: [ True  True False False False False  True False False]\n",
      "Feature Ranking: [1 1 3 7 5 2 1 6 4]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with  Recursive Feature Elimination (RFE)\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Feature Name: %s\" % nama[0:8])\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Principal Component Analysis (PCA)\n",
    "\n",
    "***Principal Component Analysis (atau PCA)*** menggunakan aljabar linier untuk mengubah dataset menjadi bentuk kompresi. Umumnya ini disebut **teknik reduksi data**. \n",
    "\n",
    "PCA membantu proses pemilihan jumlah dimensi atau *input variable* utama pada hasil *data transform*. Dalam contoh di bawah ini merupakan penggunaan PCA untuk memilih 3 *input variable* utama. Pelajari lebih lanjut tentang kelas PCA dalam scikit-learn dengan meninjau API di link berikut: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.995 0.004 0.001]\n",
      "[[ 9.684e-04 -3.395e-04 -3.958e-03 -1.000e+00 -2.831e-03  3.550e-04\n",
      "   4.278e-05  8.183e-04  5.547e-04]\n",
      " [ 8.505e-04  2.073e-04 -3.240e-03  8.600e-04 -8.448e-03  2.686e-03\n",
      "   4.114e-04  9.999e-01  5.863e-03]\n",
      " [-3.214e-02  9.081e-03  2.100e-01 -3.616e-03  9.770e-01 -3.276e-03\n",
      "   5.269e-03  8.882e-03  1.496e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with Principal Component Analysis (PCA)\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Feature Importance\n",
    "*Bagged Decision Tree* seperti ***Random Forest*** dan ***Extra Trees*** dapat digunakan untuk memperkirakan tingkat relevansi input variable terhadap output variable.  \n",
    "\n",
    "Salah satu contoh *feature importance classifier* yang umum digunakan adalah ***ExtraTreesClassifier***.\n",
    "\n",
    "Note: ***ExtraTreesClassifier*** dapat dipelajari lebih lanjut pada Scikit Learn API dengan link berikut:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier. html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV']\n",
      "[0.214 0.15  0.166 0.132 0.274 0.014 0.003 0.028 0.02 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3.7\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance menggunakan Extra Trees CLassifier\n",
    "\n",
    "# Load Library\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Load Dataset\n",
    "nama = ['PUAS', 'EVAL', 'PROJ', 'KB', 'DK', 'KK', 'PROM', 'DIV', 'GAJI', 'STAT'] \n",
    "dataset = pd.read_csv(\"datahr.csv\")\n",
    "array = dataset.values\n",
    "\n",
    "# Pemisahaan Array menjadi Array Input Variable and Array Output Variable\n",
    "X = array[:,0:9]\n",
    "Y = array[:,9]\n",
    "\n",
    "# Feature Extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "print(nama[0:8])\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variable dengan score tertinggi merupakan variable yang terpenting dalam pemodelan Output Variable. Terlihat tiga input variable yang memiliki score tertinggi adalah ***DK, PUAS, dan KB.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ringkasan\n",
    "Dalam bab ini Anda menemukan pemilihan fitur untuk menyiapkan data pembelajaran mesin dengan Python dengan scikit-learn. Anda belajar tentang 4 teknik seleksi fitur otomatis yang berbeda:\n",
    "- Univariate Selection.\n",
    "- Recursive Feature Elimination.\n",
    "- Principle Component Analysis.\n",
    "- Feature Importance\n",
    "\n",
    "Sekarang saatnya untuk mulai mencari cara mengevaluasi algoritma Machine Learning pada dataset yang digunakan. Dalam pembahasan berikut akan digunakan metode resampling yang dapat digunakan untuk memperkirakan kinerja algoritma *machine learning* pada data yang tidak terlihat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
